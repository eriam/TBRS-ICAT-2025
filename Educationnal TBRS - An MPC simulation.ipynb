{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a75817",
   "metadata": {},
   "source": [
    "# Educationnal TBRS + MPC (Human-in-the-Loop)\n",
    "\n",
    "This notebook demonstrates a **Trajectory‑Based Recommender System (TBRS)** using **Model Predictive Control (MPC)** within the framework of **control theory**.\n",
    "\n",
    "We explore a scenario for developing cybersecurity skills:\n",
    "- State $x \\in \\mathbb{R}^4$ represents skill levels in phishing, firewall, passwords, and incident response,\n",
    "- The system follows linear dynamics $x_{t+1}=Ax_t + Bu_t$,\n",
    "- The objective is to guide a student to a **target state** $x_{\\text{target}}$ by recommending appropriate **educational resources**.\n",
    "\n",
    "In this **human‑in‑the‑loop** approach, at each step, the system suggests a **Top‑k** list of items, and the learner selects an item based on a configurable choice policy, affecting the learning trajectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb9723",
   "metadata": {},
   "source": [
    "## 1) Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5eac512",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "def colvec(x):\n",
    "    \"\"\"Force into a numpy column vector of shape (n,1).\"\"\"\n",
    "    x = np.asarray(x).reshape(-1, 1)\n",
    "    return x\n",
    "\n",
    "def topk_indices(vec, k=3):\n",
    "    \"\"\"Returns indices sorted by descending value (Top-k) of a 1D vector.\"\"\"\n",
    "    v = np.asarray(vec).ravel()\n",
    "    order = np.argsort(-v)  # descending sort\n",
    "    return order[:k], v[order[:k]]\n",
    "\n",
    "def generate_B(n, m, scenario=1, seed=None):\n",
    "    \"\"\"Generate matrix B based on the chosen scenario.\"\"\"\n",
    "    levels = np.random.choice([0,1,2], size=m, p=[0.4,0.4,0.2])\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    B = np.zeros((n, m))\n",
    "    if scenario == 1:\n",
    "        # Mono-compétence: chaque ressource agit sur une seule compétence\n",
    "        for j in range(m):\n",
    "            i = j % n\n",
    "            lvl = levels[j]\n",
    "            low, high = [(0.01,0.05), (0.05,0.1), (0.1,0.2)][lvl]\n",
    "            B[i, j] = np.random.uniform(low, high)\n",
    "    elif scenario == 2:\n",
    "        # Intermédiaire: structure bandée\n",
    "        for j in range(m):\n",
    "            base_i = j % n\n",
    "            lvl = levels[j]\n",
    "            low, high = [(0.01,0.05), (0.05,0.1), (0.1,0.2)][lvl]\n",
    "            width = 1 + lvl\n",
    "            for i in range(n):\n",
    "                if abs(i - base_i) <= width // 2:\n",
    "                    B[i, j] = np.random.uniform(low, high)\n",
    "    elif scenario == 3:\n",
    "        # Multi-compétences: matrice dense\n",
    "        for j in range(m):\n",
    "            lvl = levels[j]\n",
    "            low, high = [(0.01,0.05), (0.05,0.1), (0.1,0.2)][lvl]\n",
    "            B[:, j] = np.random.uniform(low, high, n)\n",
    "    else:\n",
    "        raise ValueError(\"Scénario non reconnu. Choisir 1, 2 ou 3.\")\n",
    "\n",
    "    print(f\"Generated B with shape {B.shape} for scenario {scenario}\")\n",
    "    return B, levels\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c16faa5",
   "metadata": {},
   "source": [
    "## 2) Educational Model (A, B), Initial and Target State\n",
    "In our example, we take:\n",
    "- **A**: retention (99%) per skill, without cross-transfer,\n",
    "- **B**: 80 resources\n",
    "- **$x_0$** and **$x_{\\text{target}}$** determined.\n",
    "- **m**: Number of resources (80),\n",
    "- **n**: Number of skills (4),\n",
    "- **k**: Top-k construction parameters (8),\n",
    "- **T_p**: MPC prediction horizon (10),\n",
    "- **T_s**: Simulation horizon (50),\n",
    "- **simulate_user**: Boolean to simulate user interaction (True),\n",
    "- **Q_mpc**: State cost matrix,\n",
    "- **Qf_mpc**: Final state cost matrix."
   ]
  },
  {
   "cell_type": "code",
   "id": "1707a75b0b1d90c",
   "metadata": {},
   "source": [
    "\n",
    "A = np.eye(4) * 0.99 # Matrix A: 99% retention without cross-skill coupling\n",
    "m = 80  # Number of resources\n",
    "n = 4  # Number of skills\n",
    "k = 8 # Top-k construction parameters\n",
    "T_p = 10 # T_p: MPC prediction horizon (e.g., 10)\n",
    "T_s = 50 # T_s: Simulation horizon (e.g., 50)\n",
    "simulate_user = True\n",
    "Q_mpc = np.diag([1.0, 1.0, 1.0, 5.0])\n",
    "Qf_mpc = np.diag([1.0, 1.0, 1.0, 5.0])\n",
    "\n",
    "def R_t(t, levels):\n",
    "    R = np.eye(m)\n",
    "    for j, lvl in enumerate(levels):\n",
    "        R[j, j] = [0.01, 0.05, 0.1][lvl]\n",
    "    return R\n",
    "\n",
    "\n",
    "# Initial state of student (x0) and target (x_target)\n",
    "x0 = np.array([0.70, 0.40, 0.60, 0.30]).reshape(-1,1)\n",
    "x_target = np.array([0.95, 0.80, 0.90, 0.70]).reshape(-1,1)\n",
    "\n",
    "# Allocation of arrays for simulation\n",
    "x_sim = np.zeros((T_s + 1, n))\n",
    "x_sim[0] = x0.ravel()\n",
    "u_applied = np.zeros((T_s, m))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "topk_exploit_intro",
   "metadata": {},
   "source": [
    "## 3) Exploitation of U* to Build the Top-k and Human-in-the-Loop Simulation\n",
    "\n",
    "In this cell, we show:\n",
    "- how to build an instantaneous Top-k (u0);\n",
    "- how to build a weighted Top-k over the horizon (short-term vs long-term policy);\n",
    "- optionally, how to simulate the human‑in‑the‑loop by applying a one-hot choice at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "686bf8dc4fef33fd",
   "metadata": {},
   "source": [
    "\n",
    "def build_weight_vector(N, slope='short'):\n",
    "    \"\"\"Returns a normalized weight vector for horizon weighting.\"\"\"\n",
    "    if slope == 'short':\n",
    "        w = np.linspace(1.0, 0.1, N)\n",
    "    elif slope == 'long':\n",
    "        w = np.linspace(0.1, 1.0, N)\n",
    "    else:\n",
    "        w = np.ones(N)\n",
    "    return w / np.sum(w)\n",
    "\n",
    "\n",
    "def ask_user_choice(topk_idxs, topk_vals, simulate_user=True, randomness=0.2):\n",
    "\n",
    "    if simulate_user:\n",
    "        weights = np.linspace(1.0, 0.1, len(topk_idxs))\n",
    "        weights = (1 - randomness) * weights + randomness * np.ones_like(weights)\n",
    "        weights /= weights.sum()\n",
    "        return int(np.random.choice(topk_idxs, p=weights))\n",
    "\n",
    "    print('Suggestions (Top-k):')\n",
    "    for i, v in enumerate(topk_vals):\n",
    "        print(f'  [{i}] {resources[topk_idxs[i]]} (score={v:.3f})')\n",
    "    while True:\n",
    "        try:\n",
    "            s = input(f\"Enter local index (0..{len(topk_idxs)-1}) or Enter for top-1: \").strip()\n",
    "            if s == '':\n",
    "                return int(topk_idxs[0])\n",
    "            k_choice = int(s)\n",
    "            if 0 <= k_choice < len(topk_idxs):\n",
    "                return int(topk_idxs[k_choice])\n",
    "        except Exception:\n",
    "            print('Invalid entry, please try again.')\n",
    "\n",
    "\n",
    "def solve_mpc(x_current, T_p):\n",
    "    U = cp.Variable((m, T_p))\n",
    "    X = cp.Variable((n, T_p + 1))\n",
    "    constraints = [X[:, 0] == x_current.ravel()]\n",
    "    for t in range(T_p):\n",
    "        constraints += [X[:, t + 1] == A @ X[:, t] + B @ U[:, t]]\n",
    "        constraints += [U[:, t] >= 0, U[:, t] <= 1, cp.sum(U[:, t]) <= 1]\n",
    "    cost = 0\n",
    "    for t in range(T_p):\n",
    "        dx = X[:, t + 1] - x_target.ravel()\n",
    "        cost += cp.quad_form(dx, Q_mpc) + cp.quad_form(U[:, t], R_t(t, levels))\n",
    "    prob = cp.Problem(cp.Minimize(cost), constraints)\n",
    "    prob.solve(\n",
    "        solver=cp.OSQP,\n",
    "        verbose=False,\n",
    "        eps_abs=1e-5,\n",
    "        eps_rel=1e-5,\n",
    "        max_iter=20000\n",
    "    )\n",
    "    if prob.status not in ['optimal', 'optimal_inaccurate']:\n",
    "        raise RuntimeError('MPC not solved correctly: ' + str(prob.status))\n",
    "    return np.array(U.value)\n",
    "\n",
    "\n",
    "# ==== Simulation Loop ====\n",
    "U_applied = np.zeros((m, T_s))\n",
    "chosen_idx = []\n",
    "\n",
    "scenarios = [1, 2, 3]\n",
    "policies = [\"instant\", \"short\", \"long\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop over scenarios 1–3\n",
    "for scenario in scenarios:\n",
    "    B, levels = generate_B(4, m, scenario)\n",
    "\n",
    "    print(\"Mean coupling per skill in B:\", np.round(B.mean(axis=1), 3))\n",
    "    print(\"x_sim shape =\", x_sim.shape)\n",
    "    print(\"A shape =\", A.shape)\n",
    "    print(\"B shape =\", B.shape)\n",
    "\n",
    "    print(f\"\\n=== Scenario {scenario} ===\")\n",
    "    print(f\"Mean coupling per skill: {np.round(B.mean(axis=1), 3)}\")\n",
    "\n",
    "    print(f\"Generated B with shape {B.shape} for scenario {scenario}\")\n",
    "    print(\"Matrix B:\")\n",
    "    print(B)\n",
    "    print(\"Resource levels:\")\n",
    "    print(levels)\n",
    "\n",
    "    skills = [\"Phishing\", \"Firewall\", \"Passwords\", \"Incident\"]\n",
    "    levels_names = [\"easy\", \"medium\", \"hard\"]\n",
    "    levels_values = [0.25, 0.50, 0.75]\n",
    "\n",
    "    resources = []\n",
    "    for j in range(m):\n",
    "        dominant_skill_idx = np.argmax(B[:, j])\n",
    "        dominant_skill = skills[dominant_skill_idx]\n",
    "\n",
    "        lvl_idx = levels[j]\n",
    "        lvl_name = levels_names[lvl_idx]\n",
    "        lvl_val  = levels[lvl_idx]\n",
    "\n",
    "        resources.append(f\"r{j+1:03d}: {dominant_skill} ({lvl_name})\")\n",
    "\n",
    "    print(f\"{len(resources)} resources generated.\")\n",
    "    print(resources[:8])\n",
    "\n",
    "    print(\"\\n=== DEBUG: Matrix B and Resource Mapping ===\")\n",
    "    header = \"      \" + \" \".join([f\"{s[:8]:>10}\" for s in skills])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    n, m = B.shape\n",
    "    n, m\n",
    "\n",
    "    for chosen_policy in policies:\n",
    "        print(f\"\\n--- Policy: {chosen_policy} ---\")\n",
    "\n",
    "        for t in range(T_s):\n",
    "            # --- MPC solve for current state ---\n",
    "            U_pred = solve_mpc(x_sim[t], T_p)\n",
    "            u0 = U_pred[:, 0]\n",
    "\n",
    "            # --- Apply projection policy (Hadamard mask) ---\n",
    "            P_instant = np.zeros_like(U_pred); P_instant[:, 0] = 1\n",
    "            P_short   = np.zeros_like(U_pred); P_short[:, :T_p//2] = 1\n",
    "            P_long    = np.zeros_like(U_pred); P_long[:, T_p//2:]  = 1\n",
    "\n",
    "            # --- Policy selection ---\n",
    "            if chosen_policy == 'instant':\n",
    "                P = P_instant\n",
    "            elif chosen_policy == 'short':\n",
    "                P = P_short\n",
    "            elif chosen_policy == 'long':\n",
    "                P = P_long\n",
    "            else:\n",
    "                P = np.ones_like(U_pred)\n",
    "\n",
    "            U_masked = U_pred * P  # ⊙ Hadamard\n",
    "\n",
    "            # --- Build global scores by concatenating horizon vectors ---\n",
    "            scores_concat = U_masked.flatten(order='F')  # column-major flatten (u0,u1,...)\n",
    "            indices_concat = np.arange(U_masked.size)    # each corresponds to (resource, step)\n",
    "\n",
    "            # --- Sort to get global Top-k ---\n",
    "            topk_idx_flat = np.argsort(scores_concat)[::-1][:k]\n",
    "            topk_vals_flat = scores_concat[topk_idx_flat]\n",
    "\n",
    "            # --- Map back to (resource, step) coordinates ---\n",
    "            res_idx, step_idx = np.unravel_index(topk_idx_flat, U_masked.shape)\n",
    "\n",
    "            # --- Display Top-k aggregated over horizon ---\n",
    "            print(f\"\\nTop-{k} global (policy={chosen_policy}):\")\n",
    "            for r, s, v in zip(res_idx, step_idx, topk_vals_flat):\n",
    "                print(f\"  t={s:<2} | {resources[r]:<35} (score={v:.3f})\")\n",
    "\n",
    "            idx_order = res_idx[:k]\n",
    "            vals = topk_vals_flat[:k]\n",
    "\n",
    "            # --- User (simulated or manual) choice ---\n",
    "            choice = ask_user_choice(idx_order, vals, simulate_user=simulate_user)\n",
    "            chosen_idx.append(choice)\n",
    "\n",
    "            # --- Apply chosen resource effect ---\n",
    "            a_t = np.zeros_like(u0)\n",
    "            a_t[choice] = 1\n",
    "\n",
    "            # The control is conceptual only (used for ranking), the actual effect comes from B\n",
    "            # We apply directly the column of B for the chosen resource\n",
    "            b_effect = B[:, choice]\n",
    "\n",
    "            # --- Debug: visualize the actual control applied ---\n",
    "            print(f\"\\n[DEBUG] Step {t} — Applied resource: {resources[choice]}\")\n",
    "            print(f\"  a(t):            {a_t.astype(int)}\")                 # one-hot control\n",
    "            print(f\"  B[:, choice]:    {np.round(b_effect, 3)}\")           # resource effect on each skill\n",
    "            print(f\"  x(t):            {np.round(x_sim[t], 3)}\")           # current user state\n",
    "            print(f\"  A @ x(t):        {np.round(A @ x_sim[t], 3)}\")       # retained state\n",
    "            print(f\"  Δx:              {np.round(b_effect, 3)}\")           # change due to resource\n",
    "            print(f\"  x(t+1) predicted:{np.round(A @ x_sim[t] + b_effect, 3)}\")  # new state before clipping\n",
    "\n",
    "            # Optional: scaling factor (learning rate, exposure, etc.)\n",
    "            alpha = 1.0\n",
    "\n",
    "            x_sim[t + 1] = (A @ x_sim[t] + alpha * b_effect).clip(0, 1)\n",
    "            U_applied[:, t] = u0  # still store u0 for logs\n",
    "\n",
    "            print(f\"  x(t+1) applied:  {np.round(x_sim[t+1], 3)}\")         # actual next state\n",
    "\n",
    "            # --- Step diagnostics ---\n",
    "            dist_to_target = np.linalg.norm(x_sim[t] - x_target.ravel())\n",
    "            control_norm = np.linalg.norm(u0)\n",
    "            if t % 5 == 0:\n",
    "                print(f\"[Step {t}] ||x-x*||={dist_to_target:.3f}  ||u||={control_norm:.3f}  mean(R)={np.mean(np.diag(R_t(t, levels))):.3f}\")\n",
    "\n",
    "        print('\\nIndices chosen per time step:', chosen_idx)\n",
    "        print('Chosen resources:', [resources[i] for i in chosen_idx])\n",
    "\n",
    "        from collections import Counter\n",
    "        skill_stats = Counter([r.split(\":\")[1].split(\"(\")[0].strip() for r in [resources[i] for i in chosen_idx]])\n",
    "        print(\"\\nResource choice distribution:\")\n",
    "        for skill, count in skill_stats.items():\n",
    "            print(f\"  {skill:<10}: {count:3d} times\")\n",
    "\n",
    "        incident_idx = [i for i, r in enumerate(resources) if \"Incident\" in r]\n",
    "        print(f\"Incident resources: {len(incident_idx)}\")\n",
    "        for i in incident_idx[:10]:\n",
    "            print(f\"{resources[i]} — B[:,{i}] = {np.round(B[:, i], 3)}\")\n",
    "\n",
    "        # --- Monochrome-friendly skill plot ---\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        times = np.arange(T_s + 1)\n",
    "\n",
    "        # Define line styles for clarity in black & white\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "        target_styles = ['--', ':', '-.', '--']\n",
    "\n",
    "        for i in range(n):\n",
    "            style = line_styles[i % len(line_styles)]\n",
    "            ax.plot(times, x_sim[:, i], linestyle=style, linewidth=1.8, label=f\"Skill {skills[i]}\")\n",
    "            ax.axhline(y=x_target[i], linestyle=target_styles[i % len(target_styles)], linewidth=1)\n",
    "\n",
    "        # --- Info box in grayscale (no color) ---\n",
    "        avg_retention = np.mean(np.diag(A)) * 100\n",
    "        has_coupling = not np.allclose(A, np.diag(np.diag(A)))\n",
    "\n",
    "        A_summary = (\n",
    "            f\"A: {avg_retention:.0f}% retention, \"\n",
    "            f\"{'cross-skill coupling' if has_coupling else 'diagonal'}\"\n",
    "        )\n",
    "\n",
    "        info_text = (\n",
    "            f\"Policy: {chosen_policy}\\n\"\n",
    "            f\"Tₚ={T_p}, Tₛ={T_s}\\n\"\n",
    "            f\"{A_summary}\\n\"\n",
    "            f\"Scenario: {scenario if 'scenario' in locals() else 'N/A'}\"\n",
    "        )\n",
    "\n",
    "        ax.text(\n",
    "            1.02, 0.5, info_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=9,\n",
    "            va='center', ha='left',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc='white', ec='black', lw=0.6, alpha=0.9)\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Simulation step\")\n",
    "        ax.set_ylabel(\"Skill level\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(\"MPC Simulation — Skill trajectories and targets\")\n",
    "        ax.legend(loc='lower right', fontsize=8, frameon=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        key = f\"S{scenario}_{chosen_policy}\"\n",
    "        results[key] = {\n",
    "            \"x_sim\": x_sim.copy(),\n",
    "            \"U\": U_pred.copy(),\n",
    "            \"policy\": chosen_policy,\n",
    "            \"scenario\": scenario,\n",
    "            \"label\": f\"S{scenario}-{chosen_policy}\",\n",
    "            \"final\": x_sim[-1],\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Display of results\n",
    "\n",
    "In this cell we display the result of the simulation"
   ],
   "id": "a0b52f585a283c63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Styles for B/W rendering\n",
    "styles = {\n",
    "    \"instant\": (0, (1, 3)),   # very dotted line\n",
    "    \"short\":   (0, (5, 3)),   # medium dash\n",
    "    \"long\":    (0, (1, 1)),   # continuous line\n",
    "}\n",
    "markers = {\n",
    "    1: \"o\",    # scenario 1 = single-skill\n",
    "    2: \"s\",    # scenario 2 = mixed\n",
    "    3: \"x\",    # scenario 3 = multi\n",
    "}\n",
    "\n",
    "# === Global convergence plot ===\n",
    "plt.figure(figsize=(8, 4))\n",
    "times = np.arange(T_s + 1)\n",
    "\n",
    "for key, res in results.items():\n",
    "    scen = res[\"scenario\"]\n",
    "    pol  = res[\"policy\"]\n",
    "\n",
    "    # Euclidean distance to the target over the entire horizon\n",
    "    d = np.linalg.norm(res[\"x_sim\"] - x_target.T, axis=1)\n",
    "\n",
    "    plt.plot(\n",
    "        times,\n",
    "        d,\n",
    "        linestyle=styles[pol],\n",
    "        marker=markers[scen],\n",
    "        markevery=5,\n",
    "        linewidth=1.1,\n",
    "        label=f\"S{scen}-{pol}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Convergence Speed — Distance to Target over Time\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"‖x - x*‖₂ (global skill gap)\")\n",
    "plt.yscale(\"log\")  # highlights speed differences\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.legend(ncol=3, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== Summary Table ===\")\n",
    "print(\"Scenario | Policy   | Final State (x)\")\n",
    "print(\"-\" * 45)\n",
    "for key, res in results.items():\n",
    "    scen, pol = res[\"scenario\"], res[\"policy\"]\n",
    "    xf = np.round(res[\"x_sim\"][-1], 3)\n",
    "    print(f\"{scen:^8} | {pol:<8} | {xf}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "topk_exploit_001",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
